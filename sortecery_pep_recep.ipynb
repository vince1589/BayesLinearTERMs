{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import glob\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import sys\n",
    "import pymc3 as pm \n",
    "#Set some List\n",
    "myAA = [\"R\",\"H\",\"K\",\"D\",\"E\",\"S\",\"T\",\"N\",\"Q\",\"C\",\"G\",\"P\",\"A\",\"V\",\"I\",\"L\",\"M\",\"F\",\"Y\",\"W\"]\n",
    "FullAmino = [\"ARG\",\"HIS\",\"LYS\",\"ASP\",\"GLU\",\"SER\",\"THR\",\"ASN\",\"GLN\",\"CYS\",\"GLY\",\"PRO\",\"ALA\",\"VAL\",\"ILE\",\"LEU\",\"MET\",\"PHE\",\"TYR\",\"TRP\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load score from Etabx with some filtering coming from another file\n",
    "AllScore = pickle.load(open(\"./data/AllNormScore.pk\",\"rb\"))\n",
    "\n",
    "#Load what is the receptor contact AA identity\n",
    "ReceptorContact = pickle.load(open(\"./data/ReceptorContact.pk\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=9\n",
      "Poly=0\n",
      "Random=0\n"
     ]
    }
   ],
   "source": [
    "Dataset = 9\n",
    "Poly = 2\n",
    "Rand = 0\n",
    "RecepToRun = \"MCL1\"\n",
    "Prior = \"Struct\"\n",
    "Prior = \"No\"\n",
    "Poly = 0\n",
    "PrioStr = 1.0\n",
    "RunName = \"Test\"\n",
    "\n",
    "#[Dataset,Poly,Rand,RecepToRun,Prior,PrioStr,RunName] = sys.argv[1:]\n",
    "Dataset = int(Dataset)\n",
    "Poly = int(Poly)\n",
    "Rand = int(Rand)\n",
    "\n",
    "print(\"Dataset=%d\" % (Dataset))\n",
    "print(\"Poly=%d\" % (Poly))\n",
    "print(\"Random=%d\" % (Rand))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model=Test_9_MCL1_rand_0_poly_0_Prior_No_PrioStr_1.000\n"
     ]
    }
   ],
   "source": [
    "model = \"_\".join([RunName,str(Dataset),RecepToRun,\"rand\",str(Rand),\"poly\",str(Poly),\"Prior\",Prior,\"PrioStr\",\"%.3f\" % (PrioStr)])\n",
    "#pickle.dump(trace,open(\"/scratch/users/vfrap/BayesianModel/\"+model+\"_small.pk\",\"wb\"))\n",
    "print(\"Model=%s\" % (model))\n",
    "model_csv = \"./results/\"+model+\"_cor.csv\"\n",
    "if os.path.isfile(model_csv):\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of Feature=400\n"
     ]
    }
   ],
   "source": [
    "#Sort All Feature\n",
    "AllFeature = sorted(list(AllScore.keys()))\n",
    "\n",
    "if Poly == 0:\n",
    "    Linear = []\n",
    "    for s in AllFeature:\n",
    "        sp = s.split(\" \")\n",
    "        if sp[0] == sp[1]:\n",
    "            Linear.append(s)\n",
    "    AllFeature = Linear\n",
    "    \n",
    "if Poly == 2:\n",
    "    RecepPep = []\n",
    "    for s in AllFeature:\n",
    "        sp = s.split(\" \")\n",
    "        if sp[0].split(\",\")[-1] != sp[1].split(\",\")[-1]:\n",
    "            RecepPep.append(s)\n",
    "            #print(s)\n",
    "        if sp[0] == sp[1]:\n",
    "            RecepPep.append(s)\n",
    "    AllFeature = RecepPep\n",
    "\n",
    "print(\"This is the number of Feature=%d\" % (len(AllFeature)))\n",
    "\n",
    "#Build strc Prior\n",
    "StrcMu = []\n",
    "StrcSt = []\n",
    "for s in AllFeature:\n",
    "    StrcMu.append(np.mean(AllScore[s]))\n",
    "    StrcSt.append(np.std(AllScore[s])+PrioStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SmallRecep = \"NA\"\n",
    "Test = []\n",
    "for t in RecepToRun.split(\",\"):\n",
    "    if t == \"B2CL1\":SmallRecep = \"x\"\n",
    "    if t == \"MCL1\":SmallRecep = \"m\"\n",
    "    if t == \"B2LA1\":SmallRecep = \"f\"\n",
    "    Test.append(\"mix_\"+SmallRecep+\"_ener\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BindingData = pd.read_csv(\"./cluster/clust_\"+str(Dataset)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Encode Seq string on feature set\n",
    "def encode_FeatDict(s,SortFeatInd,RecepCon,PepStart = 91,PepChain = \"B\"):\n",
    "    vect = [0]*len(SortFeatInd)\n",
    "    for i in range(len(s)):\n",
    "        k1 = \",\".join([s[i],str(i+PepStart),PepChain])\n",
    "        #Do pep-pep\n",
    "        for j in range(i,len(s)):\n",
    "            k2 = \",\".join([s[j],str(j+PepStart),PepChain])\n",
    "            contK = \" \".join(sorted([k1,k2]))\n",
    "            if contK not in SortFeatInd:continue\n",
    "            ind = SortFeatInd[contK]\n",
    "            vect[ind] = 1\n",
    "        for k2 in RecepCon:\n",
    "            contK = \" \".join(sorted([k1,k2]))\n",
    "            if contK not in SortFeatInd:continue\n",
    "            ind = SortFeatInd[contK]\n",
    "            #print(contK,ind)\n",
    "            vect[ind] = 1\n",
    "    return(vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SortFeatInd = dict()\n",
    "for i in range(len(AllFeature)):\n",
    "    SortFeatInd[AllFeature[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,100,B A,100,B\n",
      "A,91,B A,91,B\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "#Find where peptide start\n",
    "minn = 999999\n",
    "for s in AllFeature:\n",
    "    sp = s.split(\" \")\n",
    "    if \",B\" in sp[0]:\n",
    "        num = int(sp[0].split(\",\")[1])\n",
    "        if num < minn:\n",
    "            minn = num\n",
    "            print(s)\n",
    "print(minn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mix_m_ener MCL1 182\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for (t,r) in zip(Test,RecepToRun.split(\",\")):\n",
    "    TrainDF = BindingData.dropna(subset=[t])\n",
    "    print(t,r,len(TrainDF))\n",
    "    for (seq,val) in zip(TrainDF[\"twenty\"].values,TrainDF[t].values):\n",
    "        enc = encode_FeatDict(seq,SortFeatInd,ReceptorContact[r],PepStart=minn)\n",
    "        X.append(enc)\n",
    "        Y.append(val)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./cluster/random.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_rand = []\n",
    "X_rand = []\n",
    "for r in ([\"B2CL1\",\"B2LA1\",\"MCL1\"]):\n",
    "    for (s,v) in zip(list(df[\"EncodeAA\"].values)[:int(len(df)/2)],list(df[\"Ener\"].values)[:int(len(df)/2)]):\n",
    "        enc = encode_FeatDict(seq,SortFeatInd,ReceptorContact[r],PepStart=minn)\n",
    "        X_rand.append(enc)\n",
    "        Y_rand.append(v)\n",
    "X_rand = np.array(X_rand)\n",
    "Y_rand = np.array(Y_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define alphas\n",
      "Define beta\n",
      "Define Expected\n",
      "Define Likehood\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 4999/5000 [02:14<00:00, 37.29it/s]/media/vince/anaconda/envs/py35/lib/python3.5/site-packages/pymc3/step_methods/hmc/nuts.py:467: UserWarning: Chain 0 contains 6 diverging samples after tuning. If increasing `target_accept` does not help try to reparameterize.\n",
      "  % (self._chain_id, n_diverging))\n",
      "100%|██████████| 5000/5000 [02:14<00:00, 37.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "basic_model = pm.Model()\n",
    "#Prior = \"No\"\n",
    "with basic_model:\n",
    "    print(\"Define alphas\")\n",
    "    # Priors for unknown model parameters\n",
    "    if Prior == \"No\":\n",
    "        alphas = pm.Normal('alphas', mu=0.0, sd=PrioStr,shape=len(X[0]))\n",
    "    if Prior == \"Struct\":\n",
    "        print(\"Struct Prior\")\n",
    "        alphas = pm.Normal('alphas', mu=np.array(StrcMu), sd=np.array(StrcSt),shape=len(X[0]))\n",
    "        #alphas = pm.Normal('alphas', mu=np.array(StrcMu), sd=PrioStr,shape=len(X[0]))\n",
    "    print(\"Define beta\")\n",
    "    beta = pm.Normal('beta', mu=np.mean(Y), sd=1)\n",
    "    \n",
    "    print(\"Define Expected\")\n",
    "    # Expected value of outcome\n",
    "    \n",
    "    mscale = pm.Normal('scale', mu=2, sd=1)\n",
    "    \n",
    "    mu = beta + pm.math.dot(alphas, X.transpose())*mscale\n",
    "    #mu = beta + pm.math.dot(alphas, X.transpose())\n",
    "    \n",
    "    #sigma = pm.HalfCauchy('sigma', beta=10, testval=1.)\n",
    "    \n",
    "    print(\"Define Likehood\")\n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    Y_obs = pm.Normal('Y_obs', mu=mu, sd=0.5, observed=Y)\n",
    "    \n",
    "    #Random data\n",
    "    if Rand == 1:\n",
    "        randomdata = pm.Normal('Y_rand', mu=beta + pm.math.dot(alphas, X_rand.transpose())*mscale, sd=2.0, observed=Y_rand)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "with basic_model:\n",
    "    print(\"Sampling\")\n",
    "    # Inference!\n",
    "    #start = pm.find_MAP() # Find starting value by optimization\n",
    "    #step = pm.NUTS(state=start) # Instantiate MCMC sampling algorithm\n",
    "    #trace = pm.sample(2000, start=start)\n",
    "    trace = pm.sample(tune=3000,draws=2000)\n",
    "    #trace = pm.sample(2000)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cluster/clust_0.csv B2CL1 0.02 0.10 4983\n",
      "./cluster/clust_1.csv B2CL1 0.30 0.10 345\n",
      "./cluster/clust_10.csv B2CL1 0.15 0.10 330\n",
      "./cluster/clust_11.csv B2CL1 0.04 0.10 320\n",
      "./cluster/clust_12.csv B2CL1 0.00 0.11 278\n",
      "./cluster/clust_2.csv B2CL1 0.09 0.09 787\n",
      "./cluster/clust_3.csv B2CL1 0.28 0.10 804\n",
      "./cluster/clust_4.csv B2CL1 0.03 0.09 309\n",
      "./cluster/clust_5.csv B2CL1 0.02 0.10 377\n",
      "./cluster/clust_6.csv B2CL1 0.10 0.11 348\n",
      "./cluster/clust_7.csv B2CL1 0.05 0.09 433\n",
      "./cluster/clust_8.csv B2CL1 0.18 0.12 455\n",
      "./cluster/clust_9.csv B2CL1 0.13 0.08 197\n",
      "./cluster/clust_0.csv MCL1 0.32 0.14 4491\n",
      "./cluster/clust_1.csv MCL1 0.18 0.08 334\n",
      "./cluster/clust_10.csv MCL1 0.22 0.10 236\n",
      "./cluster/clust_11.csv MCL1 0.08 0.14 278\n",
      "./cluster/clust_12.csv MCL1 0.07 0.11 256\n",
      "./cluster/clust_2.csv MCL1 0.40 0.07 703\n",
      "./cluster/clust_3.csv MCL1 0.41 0.08 730\n",
      "./cluster/clust_4.csv MCL1 0.10 0.14 271\n",
      "./cluster/clust_5.csv MCL1 0.05 0.08 382\n",
      "./cluster/clust_6.csv MCL1 0.30 0.11 244\n",
      "./cluster/clust_7.csv MCL1 0.09 0.13 455\n",
      "./cluster/clust_8.csv MCL1 0.21 0.10 420\n",
      "./cluster/clust_9.csv MCL1 0.58 0.05 182\n",
      "./cluster/clust_0.csv B2LA1 0.12 0.09 3805\n",
      "./cluster/clust_1.csv B2LA1 0.15 0.10 264\n",
      "./cluster/clust_10.csv B2LA1 0.13 0.12 263\n",
      "./cluster/clust_11.csv B2LA1 0.04 0.12 200\n",
      "./cluster/clust_12.csv B2LA1 0.06 0.14 190\n",
      "./cluster/clust_2.csv B2LA1 0.22 0.07 751\n",
      "./cluster/clust_3.csv B2LA1 0.27 0.09 614\n",
      "./cluster/clust_4.csv B2LA1 0.03 0.13 162\n",
      "./cluster/clust_5.csv B2LA1 0.03 0.13 280\n",
      "./cluster/clust_6.csv B2LA1 0.17 0.12 263\n",
      "./cluster/clust_7.csv B2LA1 0.06 0.10 305\n",
      "./cluster/clust_8.csv B2LA1 0.02 0.12 335\n",
      "./cluster/clust_9.csv B2LA1 0.37 0.08 178\n"
     ]
    }
   ],
   "source": [
    "TrainRecepToRun = RecepToRun\n",
    "AllCors = dict()\n",
    "c = 0\n",
    "for RecepToRun in [\"B2CL1\",\"MCL1\",\"B2LA1\"]:\n",
    "    for f in sorted(glob.glob(\"./cluster/clust_*.csv\")):\n",
    "        BindingData = pd.read_csv(f)\n",
    "    \n",
    "        SmallRecep = \"NA\"\n",
    "        if RecepToRun == \"B2CL1\":SmallRecep = \"x\"\n",
    "        if RecepToRun == \"MCL1\":SmallRecep = \"m\"\n",
    "        if RecepToRun == \"B2LA1\":SmallRecep = \"f\"\n",
    "        Test = \"mix_\"+SmallRecep+\"_ener\"\n",
    "        TrainDF = BindingData.dropna(subset=[Test])\n",
    "        X = []\n",
    "        Y = []\n",
    "        for (seq,val) in zip(TrainDF[\"twenty\"].values,TrainDF[Test].values):\n",
    "            enc = encode_FeatDict(seq,SortFeatInd,ReceptorContact[RecepToRun],PepStart=minn)\n",
    "            X.append(enc)\n",
    "            Y.append(val)\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        AllCor = []\n",
    "        for i in range(1000,2000):\n",
    "            w = trace.get_values(\"alphas\")[i]\n",
    "            m = trace.get_values(\"scale\")[i]\n",
    "            b = trace.get_values(\"beta\")[i]\n",
    "            \n",
    "            xi = np.dot(w, X.transpose())*m+b\n",
    "            yi = Y\n",
    "            \n",
    "            cor = np.corrcoef(xi,yi)[0][1]\n",
    "            AllCor.append(cor)\n",
    "        c = c+1\n",
    "        AllCors[c] = dict()\n",
    "        AllCors[c][\"File\"] = f\n",
    "        AllCors[c][\"RecepToRun\"] = RecepToRun\n",
    "        AllCors[c][\"MeanCor\"] = np.mean(AllCor)\n",
    "        AllCors[c][\"StdCor\"] = np.std(AllCor)\n",
    "        AllCors[c][\"Dataset\"] = Dataset\n",
    "        AllCors[c][\"ReceptorSet\"] = TrainRecepToRun\n",
    "        AllCors[c][\"Random\"] = Rand\n",
    "        AllCors[c][\"Poly\"] = Poly\n",
    "        AllCors[c][\"Model\"] = model\n",
    "        AllCors[c][\"Prior\"] = Prior\n",
    "        AllCors[c][\"PriorStr\"] = PrioStr\n",
    "        AllCors[c][\"Test\"] = RunName\n",
    "        print(f,RecepToRun,\"%.2f %.2f\" % (np.mean(AllCor),np.std(AllCor)),len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_csv = \"./results/\"+model+\"_cor.csv\"\n",
    "pd.DataFrame(AllCors).to_csv(model_csv )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/users/vfrap/BayesianModel/Test_9_MCL1_rand_0_poly_0_Prior_No_PrioStr_1.000.pk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8df3c7d7ddea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#This file is big, write somewhere with lots of space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/scratch/users/vfrap/BayesianModel/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pk\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch/users/vfrap/BayesianModel/Test_9_MCL1_rand_0_poly_0_Prior_No_PrioStr_1.000.pk'"
     ]
    }
   ],
   "source": [
    "#This file is big, write somewhere with lots of space\n",
    "model_pickle = \"/scratch/users/vfrap/BayesianModel/\"+model+\".pk\"\n",
    "pickle.dump(trace,open(model_pickle,\"wb\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
